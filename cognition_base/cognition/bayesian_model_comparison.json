[
    {
        "DESIGN_INSIGHT": "### Bayesian Model Comparison Using WAIC and LOO Cross-Validation",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "**Model_Selection_Success_Signatures**:\n- WAIC and LOO values provide consistent ranking of competing models with clear differences\n- Standard errors of information criteria allow meaningful comparison between models\n- Model weights derived from information criteria sum to 1 and indicate relative support\n- Selected models show superior out-of-sample predictive performance on held-out data\n- Posterior predictive checks confirm selected model captures key data features better than alternatives\n- Pareto-k diagnostic values below 0.7 indicate reliable LOO approximation\n**Warning_Indicators_For_Model_Comparison**:\n- WAIC and LOO rankings disagree substantially suggesting model comparison uncertainty\n- Large standard errors relative to differences make model selection inconclusive\n- High Pareto-k values (>0.7) indicate unreliable LOO approximation requiring K-fold CV\n- Effective number of parameters (p_WAIC) approaches or exceeds sample size indicating overfitting\n- Selected model fails posterior predictive checks despite better information criteria",
        "BACKGROUND": "**Context**: Bayesian model comparison requires principled methods to assess out-of-sample predictive performance while accounting for model complexity. Traditional approaches like AIC/BIC have limitations in Bayesian settings.\n**Historical_Development**: Watanabe developed WAIC (2010) as Bayesian generalization of AIC. Vehtari et al. (2017) improved LOO implementation with Pareto smoothing. Both provide practical alternatives to marginal likelihood computation.\n**Key_Advantage**: Information criteria approximate cross-validation without refitting models, providing computationally efficient model comparison with uncertainty quantification.",
        "ALGORITHMIC_INNOVATION": "**Core_Model_Comparison_Methods**:\n- **WAIC (Widely Applicable Information Criterion)**: Estimates out-of-sample predictive performance using posterior draws\n- **LOO (Leave-One-Out Cross-Validation)**: Approximates full cross-validation using importance sampling\n- **Model Stacking**: Combines models using optimal weights for predictive performance\n- **Pseudo-BMA**: Bayesian model averaging using information criteria weights\n**Mathematical_Framework**:\n- WAIC = -2(lppd - p_WAIC) where lppd = ∑log(∫p(yᵢ|θ)p(θ|y)dθ) and p_WAIC = ∑Var(log p(yᵢ|θ))\n- LOO = -2∑log p̂(yᵢ|y₋ᵢ) where p̂(yᵢ|y₋ᵢ) uses Pareto smoothed importance sampling\n- Model weights: exp(-½Δᵢ)/∑exp(-½Δⱼ) where Δᵢ is information criterion difference from best model\n**Computational_Properties**: Both methods scale linearly with sample size and number of posterior draws; LOO provides additional diagnostic information through Pareto-k values.",
        "IMPLEMENTATION_GUIDANCE": "**PyMC_Model_Comparison_Workflow**:\n```python\n# Fit competing models with log-likelihood stored\nwith pm.Model() as model1:\n    # Model 1 specification\n    trace1 = pm.sample(2000, return_inferencedata=True, \n                       idata_kwargs={'log_likelihood': True})\n\nwith pm.Model() as model2:\n    # Model 2 specification  \n    trace2 = pm.sample(2000, return_inferencedata=True,\n                       idata_kwargs={'log_likelihood': True})\n\n# Compute information criteria\nwaic1 = az.waic(trace1)\nwaic2 = az.waic(trace2)\nloo1 = az.loo(trace1)\nloo2 = az.loo(trace2)\n\n# Model comparison\nmodel_comparison = az.compare({\n    'Model1': trace1,\n    'Model2': trace2\n})\n\nprint(\"Model Comparison Results:\")\nprint(model_comparison)\n\n# Check LOO reliability\nprint(f\"\\nModel 1 Pareto-k > 0.7: {(loo1.pareto_k > 0.7).sum().item()}\")\nprint(f\"Model 2 Pareto-k > 0.7: {(loo2.pareto_k > 0.7).sum().item()}\")\n\n# Model stacking weights\nstacking_weights = az.compare(traces_dict, method='stacking')['weight']\nprint(f\"\\nStacking weights: {stacking_weights.values}\")\n```\n**Interpretation_Guidelines**: Lower WAIC/LOO indicates better predictive performance; differences >2 standard errors suggest meaningful model differences; Pareto-k >0.7 requires K-fold CV.",
        "DESIGN_AI_INSTRUCTIONS": "**Agent_Model_Comparison_Protocol**:\n- **Mandatory_Storage**: Always store log-likelihood during sampling for model comparison\n- **Comparison_Threshold**: Consider models meaningfully different if WAIC/LOO difference exceeds 2 standard errors\n- **Reliability_Check**: Verify Pareto-k diagnostics; use K-fold CV if >10% of observations have k>0.7\n- **Multi-Model_Strategy**: When models are similar (difference <2 SE), consider model averaging or stacking\n- **Validation_Requirement**: Confirm selected model passes posterior predictive checks\n- **Reporting_Standards**: Report information criteria with standard errors and model weights; include Pareto-k diagnostic summary"
    },
    {
        "DESIGN_INSIGHT": "### Marginal Likelihood Estimation for Bayesian Model Evidence",
        "EXPERIMENTAL_TRIGGER_PATTERNS": "**Marginal_Likelihood_Success_Signatures**:\n- Bridge sampling converges to stable marginal likelihood estimates with small Monte Carlo error\n- Bayes factors provide clear evidence ratios between competing models (BF > 3 or BF < 1/3)\n- Posterior model probabilities sum to 1 and indicate relative model support\n- Results consistent across different marginal likelihood estimation methods\n- Model evidence aligns with out-of-sample predictive performance\n**Computational_Challenge_Indicators**:\n- Bridge sampling fails to converge or produces unstable estimates across runs\n- Harmonic mean estimator gives unreliable results with high variance\n- Nested sampling requires excessive computational time for high-dimensional models\n- Marginal likelihood estimates vary dramatically with prior specification\n- Bayes factors are sensitive to minor model specification changes",
        "BACKGROUND": "**Context**: Marginal likelihood p(y|M) = ∫p(y|θ,M)p(θ|M)dθ provides direct measure of model evidence for Bayesian model comparison. Unlike information criteria, it accounts for full parameter uncertainty.\n**Computational_Challenge**: Marginal likelihood computation is notoriously difficult due to high-dimensional integration over parameter space. Many naive approaches (like harmonic mean estimator) are unstable.\n**Modern_Solutions**: Bridge sampling, nested sampling, and path sampling provide more reliable marginal likelihood estimates, though at higher computational cost than WAIC/LOO.",
        "ALGORITHMIC_INNOVATION": "**Core_Marginal_Likelihood_Methods**:\n- **Bridge Sampling**: Uses importance sampling bridge between posterior and reference distribution\n- **Nested Sampling**: Explores parameter space while maintaining likelihood constraints\n- **Path Sampling**: Integrates over path connecting prior and posterior distributions\n- **Stepping Stone Sampling**: Discrete approximation to path sampling using multiple intermediates\n**Mathematical_Framework**:\n- Marginal likelihood: p(y|M) = ∫p(y|θ,M)p(θ|M)dθ\n- Bridge sampling estimate: p̂(y) = (1/n₁)∑[p(y|θᵢ)p(θᵢ)/q(θᵢ)] / (1/n₂)∑[1/q(θⱼ)]\n- Bayes factor: BF₁₂ = p(y|M₁)/p(y|M₂)\n- Posterior model probability: p(Mᵢ|y) = p(y|Mᵢ)p(Mᵢ)/∑p(y|Mⱼ)p(Mⱼ)\n**Computational_Requirements**: Bridge sampling needs posterior samples and good reference distribution; nested sampling requires specialized samplers.",
        "IMPLEMENTATION_GUIDANCE": "**PyMC_Marginal_Likelihood_Implementation**:\n```python\n# Bridge sampling for marginal likelihood\nimport arviz as az\nfrom scipy import stats\n\n# Fit model with posterior samples\nwith pm.Model() as model:\n    # Model specification\n    trace = pm.sample(4000, chains=4, return_inferencedata=True)\n\n# Bridge sampling (requires arviz integration)\ntry:\n    # This requires additional packages like pymultinest or dynesty\n    marginal_likelihood = az.stats.marginal_likelihood(trace, model)\n    print(f\"Log marginal likelihood: {marginal_likelihood}\")\nexcept:\n    print(\"Bridge sampling not available; use WAIC/LOO instead\")\n\n# Alternative: Use SMC for marginal likelihood approximation\nwith model:\n    smc_trace = pm.sample_smc(2000, chains=4)\n    log_marginal_likelihood = smc_trace.report.log_marginal_likelihood\n    print(f\"SMC log marginal likelihood: {log_marginal_likelihood}\")\n\n# Bayes factor computation\ndef compute_bayes_factor(log_ml1, log_ml2):\n    \"\"\"Compute Bayes factor from log marginal likelihoods\"\"\"\n    return np.exp(log_ml1 - log_ml2)\n\nbf = compute_bayes_factor(log_ml1, log_ml2)\nprint(f\"Bayes factor (Model 1 vs Model 2): {bf:.3f}\")\n\n# Interpretation\nif bf > 3:\n    print(\"Substantial evidence for Model 1\")\nelif bf < 1/3:\n    print(\"Substantial evidence for Model 2\")\nelse:\n    print(\"Inconclusive evidence\")\n```\n**Implementation_Notes**: SMC sampling in PyMC provides marginal likelihood estimates; bridge sampling requires additional packages; use WAIC/LOO for routine model comparison.",
        "DESIGN_AI_INSTRUCTIONS": "**Agent_Marginal_Likelihood_Strategy**:\n- **Primary_Method**: Use WAIC/LOO for routine model comparison due to computational efficiency and reliability\n- **Marginal_Likelihood_Cases**: Reserve for formal Bayesian model selection when prior model probabilities are meaningful\n- **SMC_Integration**: Use SMC sampling when marginal likelihood needed; report log marginal likelihood with Monte Carlo standard error\n- **Bayes_Factor_Interpretation**: BF>3 substantial evidence, BF>10 strong evidence, BF>100 decisive evidence\n- **Sensitivity_Analysis**: Test robustness of Bayes factors to prior specification changes\n- **Computational_Tradeoff**: Balance computational cost against inferential benefits; WAIC/LOO often sufficient for practical model selection"
    }
]